{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このチュートリアルでは、「畳み込みニューラルネットワーク (Convolutional Neural Network:CNN)を用いた画像認識タスク」をテーマに取り上げ、CNTK でどのように実装するのかについて、step by step で学んでいただくことを目的にしています。\n",
    "\n",
    "1. ディープラーニングのワークフローを順を追って見ていき、CNTK の Python API でそれぞれどう実装していくのかを見ていきます。\n",
    "    1. reader (入力データの定義)\n",
    "    2. network (モデルを定義)\n",
    "    3. trainer (モデルをどう学習するかを定義)\n",
    "\n",
    "2. 一通りワークフローの流れを理解したら、次に、各種パラメータを変更したり、学習テクニックなどを使ってみたり、あるいは、新しくネットワークを\n",
    "自分自身で設計するなどしてより高い認識精度が出せるようにチャレンジしていただきます。\n",
    "\n",
    "\n",
    "\n",
    "# 使用するデータセット\n",
    "\n",
    "本ハンズオンでは、画像認識タスクのベンチマークとしてよく使われる「CIFAR-10」という画像データセットを使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Figure 1\n",
    "Image(url=\"https://cntk.ai/jup/201/cifar-10.png\", width=500, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CIFAR-10は、約 8000 万枚の画像がある 80 Million Tiny Images \n",
    "からサブセットとして約 6 万枚の画像を抽出してラベル付けしたものです。\n",
    "CIFAR-10 は、上図のように、10 クラスがラベル付けされており、訓練画像: 50000枚\n",
    "(各クラス 5000枚)、テスト画像: 10000枚(各クラス 1000枚) に分割されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Image(url=\"https://cntk.ai/jup/201/CNN.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import PIL\n",
    "import sys\n",
    "#from cntk.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 1. データ読み取り (reader)\n",
    "さて、それではこれからディープラーニングのワークフローを順を追って見ていきましょう。\n",
    "まず、モデルが学習するためには、一般的に下記に示すような、各画像とその画像の正解ラベルが対応しているようなフォーマットを\n",
    "入力データ (ここで map text fileと呼ぶ)として想定しています。\n",
    "※ 生のデータセットからモデルが学習しやすい以下のようなフォーマットに事前に変換すること前処理と呼びます。\n",
    "\n",
    "\n",
    "<画像データのフルパス> <タブ> <正解ラベル>\n",
    "\n",
    "Example of a map text file:\n",
    "\n",
    "    S:\\data\\CIFAR-10\\train\\00001.png\t9\n",
    "    S:\\data\\CIFAR-10\\train\\00002.png\t9\n",
    "    S:\\data\\CIFAR-10\\train\\00003.png\t4\n",
    "    S:\\data\\CIFAR-10\\train\\00004.png\t1\n",
    "    S:\\data\\CIFAR-10\\train\\00005.png\t1\n",
    "\n",
    "また、CNN で画像を学習させる際、学習画像をずらしたりぼかしたり、色々と変形を加えるなどして学習画像を増やす (このことを data augmentationと呼びます) ことで、認識をロバスト(頑健性を高める) にするというテクニックが一般的によく使われています。\n",
    "\n",
    "Data Augmentation の例：\n",
    "* アスペクト比の変更\n",
    "* 回転 (ロール・ピッチ)\n",
    "* ぼかし\n",
    "* ノイズ付与\n",
    "* ずらし ・・・etc.\n",
    "\n",
    "CNTK 上で画像データを読み取る際は、1. map text file を取ってきて、2. 画像データに対して data augmentation をかける、といった処理\n",
    "を平行して行えるような reader を作ります。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cntk.io import MinibatchSource, ImageDeserializer, StreamDef, StreamDefs\n",
    "import cntk.io.transforms as xforms \n",
    "\n",
    "\n",
    "\n",
    "# model dimensions\n",
    "image_height = 32\n",
    "image_width  = 32\n",
    "num_channels = 3\n",
    "num_classes  = 10\n",
    "\n",
    "\n",
    "#\n",
    "# Define the reader for both training and evaluation action.\n",
    "#\n",
    "def create_reader(map_file, mean_file, train):\n",
    "    print(\"Reading map file:\", map_file)\n",
    "    print(\"Reading mean file:\", mean_file)\n",
    "\n",
    "    # transformation pipeline for the features has jitter/crop only when training\n",
    "    transforms = []\n",
    "    if train:\n",
    "        transforms += [\n",
    "            xforms.crop(crop_type='randomside', side_ratio=0.8, jitter_type='uniRatio') # train uses data augmentation (translation only)\n",
    "        ]\n",
    "    transforms += [\n",
    "        xforms.scale(width=image_width, height=image_height, channels=num_channels, interpolations='linear'),\n",
    "        xforms.mean(mean_file)\n",
    "    ]\n",
    "    # deserializer\n",
    "    return MinibatchSource(ImageDeserializer(map_file, StreamDefs(\n",
    "        features = StreamDef(field='image', transforms=transforms), # first column in map file is referred to as 'image'\n",
    "        labels   = StreamDef(field='label', shape=num_classes)      # and second as 'label'\n",
    "    )))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. モデルの定義 (network)\n",
    "続いて、モデルの定義をするパートです。\n",
    "上図で示している CNN をストレートに実装したものが以下になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from cntk.layers import Convolution, MaxPooling, Dense\n",
    "from cntk.initializer import glorot_uniform\n",
    "from cntk.ops import relu\n",
    "\n",
    "def create_model(input, out_dims):\n",
    "    \n",
    "    net = Convolution((5,5), 32, init=glorot_uniform(), activation=relu, pad=True)(input)\n",
    "    net = MaxPooling((3,3), strides=(2,2))(net)\n",
    "\n",
    "    net = Convolution((5,5), 32, init=glorot_uniform(), activation=relu, pad=True)(net)\n",
    "    net = MaxPooling((3,3), strides=(2,2))(net)\n",
    "\n",
    "    net = Convolution((5,5), 64, init=glorot_uniform(), activation=relu, pad=True)(net)\n",
    "    net = MaxPooling((3,3), strides=(2,2))(net)\n",
    "    \n",
    "    net = Dense(64, init=glorot_uniform())(net)\n",
    "    net = Dense(out_dims, init=glorot_uniform(), activation=None)(net)\n",
    "    \n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers ライブラリ を使ってモデルを定義しよう\n",
    "\n",
    "CNTK には、簡潔にネットワークを定義するために、一般的な (例えば、全結合層や convolution 層など) ニューラルネットワークにおける各層が \"Layers\" と呼ばれるライブラリの形で事前に定義されています。Layers は関数オブジェクトとして定義されているため、とても巨大で複雑な**関数の集合体**であるニューラルネットワークをより直感的に書くことができます。\n",
    "\n",
    "\n",
    "## Sequential()\n",
    "Sequential() は、ネットワークをコンパクトに書く上で非常に便利なオペレーションです。\n",
    "\n",
    "\n",
    "例えば、隠れ層のユニット数 2048、出力層のユニット 9000で、活性化関数がすべてsigmoid 関数である  \n",
    "4層順伝播型ニューラルネットワークを実装しようとした場合、以下のように書くことができます。\n",
    "\n",
    "\n",
    "```py\n",
    "my_model = Sequential ([\n",
    "# 活性化関数に sigmoid を使う 4 つの隠れ層\n",
    "    Dense(2048, activation=sigmoid),  \n",
    "    Dense(2048, activation=sigmoid),\n",
    "    Dense(2048, activation=sigmoid),\n",
    "    Dense(2048, activation=sigmoid),\n",
    "    # 最後の層(出力層) は活性化関数が softmax\n",
    "    Dense(9000, activation=softmax)   \n",
    "])\n",
    "```\n",
    "\n",
    "## For()\n",
    "\n",
    "For() は、定義した lambda 式 をリピート実行することよって、系列モデルを構築することができます。\n",
    "\n",
    "For() を使うと先ほどの 4層順伝播型ニューラルネットワークもよりコンパクトに書けます。\n",
    "\n",
    "```py\n",
    "\n",
    "with default_options(activation=sigmoid):\n",
    "    my_model = Sequential([\n",
    "        For(range(4), lambda: Dense(2048)),\n",
    "        Dense(9000, activation=softmax)\n",
    "    ])\n",
    "      \n",
    "```\n",
    "\n",
    "\n",
    "## Sequential() や For() を使って、CNN モデルを書いてみよう！\n",
    "今紹介した便利な関数 Sequential() や For() を使って上記のCNN (create_model() )を以下に書き換えてみましょう！\n",
    "\n",
    "以下のPython API Reference に各モジュールの使い方がより詳細に記載されているので躓いたら参考にしてみてください。\n",
    "\n",
    "[参考情報] Python API Reference \n",
    "https://www.cntk.ai/pythondocs/cntk.html#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_basic_model(input, out_dims):\n",
    "\n",
    "    \n",
    "    `````````````````\n",
    "    ここに書いてみよう\n",
    "    `````````````````\n",
    "\n",
    "    return model(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. trainer (and evaluator)\n",
    "\n",
    "## Trainer を使って、モデルを学習しよう\n",
    "\n",
    "Trainer は学習に必要な全てのものをカプセル化するクラスです。それは主に以下のような\n",
    "ものを保持します。\n",
    "    \n",
    "* モデル (network パートで定義したモデル)\n",
    "* 指標: 損失関数 (loss function) と 分類誤差 (clasification error)\n",
    "* 学習のハイパーパラメータ\n",
    "    * 学習率\n",
    "    * 学習アルゴリズム\n",
    "    * 正則化項\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "**[復習]** ニューラルネットワークの学習\n",
    "1. 訓練データの中からランダムに一部のデータ(ミニバッチ)を選び出しモデルに渡す (MinibachSource クラスの next_minibatch())\n",
    "2. ミニバッチを入力層→出力層へと順番に伝播させていき、最後の出力層で値を出力する\n",
    "3. 出力結果と正解ラベルを比較し、損失関数を減らすために、各重みパラメータの勾配を計算する\n",
    "4. 重みパラメータを勾配方向に微小量だけ更新する (trainer クラスの train_minibatch())\n",
    "5. 1-4 を繰り返す\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Train and evaluate the network.\n",
    "#\n",
    "\n",
    "from cntk import momentum_sgd, learning_rate_schedule, UnitType, momentum_as_time_constant_schedule\n",
    "from cntk import Trainer\n",
    "from cntk import cross_entropy_with_softmax, classification_error, input_variable, softmax, element_times\n",
    "from cntk.logging import ProgressPrinter, log_number_of_parameters\n",
    "\n",
    "\n",
    "def train_and_evaluate(reader_train, reader_test, max_epochs, model_func):\n",
    "    \n",
    "    # Input variables denoting the features and label data\n",
    "    input_var = input_variable((num_channels, image_height, image_width))\n",
    "    label_var = input_variable((num_classes))\n",
    "\n",
    "    # Normalize the input\n",
    "    feature_scale = 1.0 / 256.0\n",
    "    input_var_norm = element_times(feature_scale, input_var)\n",
    "    \n",
    "    # apply model to input\n",
    "    z = model_func(input_var_norm, out_dims=10)\n",
    "\n",
    "    #\n",
    "    # Training action\n",
    "    #\n",
    "\n",
    "    # loss and metric\n",
    "    ce = cross_entropy_with_softmax(z, label_var)\n",
    "    pe = classification_error(z, label_var)\n",
    "\n",
    "    # training config\n",
    "    epoch_size     = 50000\n",
    "    minibatch_size = 64\n",
    "\n",
    "    # Set training parameters\n",
    "\n",
    "    lr_per_minibatch       = learning_rate_schedule([0.01]*10 + [0.003]*10 + [0.001], UnitType.minibatch, epoch_size)\n",
    "    momentum_time_constant = momentum_as_time_constant_schedule(-minibatch_size/np.log(0.9))\n",
    "    l2_reg_weight          = 0.001\n",
    "    \n",
    "    # trainer object\n",
    "    learner = momentum_sgd(z.parameters, \n",
    "                           lr = lr_per_minibatch, momentum = momentum_time_constant, \n",
    "                           l2_regularization_weight=l2_reg_weight)\n",
    "    progress_printer = ProgressPrinter(tag='Training', num_epochs=max_epochs)\n",
    "    trainer = Trainer(z, (ce, pe), [learner], [progress_printer])\n",
    "\n",
    "    # define mapping from reader streams to network inputs\n",
    "    input_map = {\n",
    "        input_var: reader_train.streams.features,\n",
    "        label_var: reader_train.streams.labels\n",
    "    }\n",
    "\n",
    "    log_number_of_parameters(z) ; print()\n",
    "\n",
    "    # Get minibatches of image to train with and perform model training\n",
    "    batch_index = 0\n",
    "    plot_data = {'batchindex':[], 'loss':[], 'error':[]}\n",
    "    for epoch in range(max_epochs):       # loop over epochs\n",
    "        sample_count = 0\n",
    "        while sample_count < epoch_size:  # loop over minibatches in the epoch\n",
    "            data = reader_train.next_minibatch(min(minibatch_size, epoch_size - sample_count), input_map=input_map) # fetch minibatch.\n",
    "            trainer.train_minibatch(data)                                   # update model with it\n",
    "            sample_count += data[label_var].num_samples                     # count samples processed so far\n",
    "            \n",
    "            # For visualization...            \n",
    "            plot_data['batchindex'].append(batch_index)\n",
    "            plot_data['loss'].append(trainer.previous_minibatch_loss_average)\n",
    "            plot_data['error'].append(trainer.previous_minibatch_evaluation_average)\n",
    "            \n",
    "            batch_index += 1\n",
    "        trainer.summarize_training_progress()\n",
    "        \n",
    "    #\n",
    "    # Evaluation action\n",
    "    #\n",
    "    epoch_size     = 10000\n",
    "    minibatch_size = 16\n",
    "\n",
    "    # process minibatches and evaluate the model\n",
    "    metric_numer    = 0\n",
    "    metric_denom    = 0\n",
    "    sample_count    = 0\n",
    "    minibatch_index = 0\n",
    "\n",
    "    while sample_count < epoch_size:\n",
    "        current_minibatch = min(minibatch_size, epoch_size - sample_count)\n",
    "\n",
    "        # Fetch next test min batch.\n",
    "        data = reader_test.next_minibatch(current_minibatch, input_map=input_map)\n",
    "\n",
    "        # minibatch data to be trained with\n",
    "        metric_numer += trainer.test_minibatch(data) * current_minibatch\n",
    "        metric_denom += current_minibatch\n",
    "\n",
    "        # Keep track of the number of samples processed so far.\n",
    "        sample_count += data[label_var].num_samples\n",
    "        minibatch_index += 1\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Final Results: Minibatch[1-{}]: errs = {:0.1f}% * {}\".format(minibatch_index+1, (metric_numer*100.0)/metric_denom, metric_denom))\n",
    "    print(\"\")\n",
    "    \n",
    "    # Visualize training result:\n",
    "    window_width            = 32\n",
    "    loss_cumsum             = np.cumsum(np.insert(plot_data['loss'], 0, 0)) \n",
    "    error_cumsum            = np.cumsum(np.insert(plot_data['error'], 0, 0)) \n",
    "\n",
    "    # Moving average.\n",
    "    plot_data['batchindex'] = np.insert(plot_data['batchindex'], 0, 0)[window_width:]\n",
    "    plot_data['avg_loss']   = (loss_cumsum[window_width:] - loss_cumsum[:-window_width]) / window_width\n",
    "    plot_data['avg_error']  = (error_cumsum[window_width:] - error_cumsum[:-window_width]) / window_width\n",
    "    \n",
    "    plt.figure(1)\n",
    "    plt.subplot(211)\n",
    "    plt.plot(plot_data[\"batchindex\"], plot_data[\"avg_loss\"], 'b--')\n",
    "    plt.xlabel('Minibatch number')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Minibatch run vs. Training loss ')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    plt.subplot(212)\n",
    "    plt.plot(plot_data[\"batchindex\"], plot_data[\"avg_error\"], 'r--')\n",
    "    plt.xlabel('Minibatch number')\n",
    "    plt.ylabel('Label Prediction Error')\n",
    "    plt.title('Minibatch run vs. Label Prediction Error ')\n",
    "    plt.show()\n",
    "    \n",
    "    return softmax(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習/テストを開始する\n",
    "上で作った train_and_evaluate() に対し、入力データ(CIFAR-10)を create_reader 経由で渡して、\n",
    "学習およびテストしてみましょう。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_path = os.path.join('data', 'CIFAR-10')\n",
    "reader_train = create_reader(os.path.join(data_path, 'train_map.txt'), os.path.join(data_path, 'CIFAR-10_mean.xml'), True)\n",
    "reader_test  = create_reader(os.path.join(data_path, 'test_map.txt'), os.path.join(data_path, 'CIFAR-10_mean.xml'), False)\n",
    "\n",
    "pred = train_and_evaluate(reader_train, reader_test, max_epochs=10, model_func=create_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 学習済みモデルを使った予測\n",
    "先ほど学習したモデルを使って実際にいつくつかのテスト画像を分類させてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Figure 6\n",
    "Image(url=\"https://cntk.ai/jup/201/00014.png\", width=64, height=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval(pred_op, image_path):\n",
    "    label_lookup = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n",
    "    image_mean   = 133.0\n",
    "    image_data   = np.array(PIL.Image.open(image_path), dtype=np.float32)\n",
    "    image_data  -= image_mean\n",
    "    image_data   = np.ascontiguousarray(np.transpose(image_data, (2, 0, 1)))\n",
    "    \n",
    "    result       = np.squeeze(pred_op.eval({pred_op.arguments[0]:[image_data]}))\n",
    "    \n",
    "    # Return top 3 results:\n",
    "    top_count = 3\n",
    "    result_indices = (-np.array(result)).argsort()[:top_count]\n",
    "    \n",
    "    \n",
    "    print(\"Top 3 predictions:\")\n",
    "    for i in range(top_count):\n",
    "        print(\"\\tLabel: {:10s}, confidence: {:.2f}%\".format(label_lookup[result_indices[i]], result[result_indices[i]] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eval(pred, \"data/CIFAR-10/test/00014.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Dropout\n",
    "Dropout は、過学習を抑制するために提唱された手法です。\n",
    "過学習 (Overfitting) とは、機械学習において、訓練データに対して学習されるが、未知のデータに対して適合できていない (汎化できていない) 状態を指します。特にニューラルネットワークのようにパラメータを大量に持ち、自由度の高いモデルは **過学習に陥りやすい** と言われています。\n",
    "\n",
    "Dropout では、ニューラルネットワークを学習する際に、ノードをランダムに消去しながら学習する手法です。ある更新で隠れ層のノードのうちいくつかを消去して学習を行い、次の更新では別のノードを消去して学習を行うことを繰り返します。これにより学習時におけるニューラルネットワークの自由度を小さくして汎化性能を上げ、過学習を抑制することができます。\n",
    "\n",
    "◇ Tips\n",
    "\n",
    "隠れ層においては、一般的に 50% 程度のノードを消去すると良いと言われています。また、当初 Dropout は全結合層のみに適用されていましたが、最近の研究では、畳み込み層などにも適用しても同様に性能を向上させることが確認されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_basic_model_with_dropout(input, out_dims):\n",
    "\n",
    "    with default_options(activation=relu):\n",
    "        model = Sequential([\n",
    "            For(range(3), lambda i: [\n",
    "                Convolution((5,5), [32,32,64][i], init=glorot_uniform(), pad=True),\n",
    "                Dropout(0.5),\n",
    "                MaxPooling((3,3), strides=(2,2))\n",
    "            ]),\n",
    "            Dense(64, init=glorot_uniform()),\n",
    "            Dropout(0.5),\n",
    "            Dense(out_dims, init=glorot_uniform(), activation=None)\n",
    "        ])\n",
    "\n",
    "    return model(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG\n",
    "VGG は、畳み込み層とプーリング層から構成されるシンプルな CNN です。ただし、重みのある層 (畳み込み層や全結合層)を全部で 16 層 (もしくは 19　層)まで重ねてディープ　にしています (層の深さに応じて、「VGG16」や「VGG19」と呼ばれます)。\n",
    "VGG では、フィルターサイズ 3x3 の畳み込み層　+ Max Pooling を数回繰り返し後、全結合層を経由して結果を出力します。\n",
    "VGG は性能が高いだけでなく、このようにとてもシンプルな構成で応用性が高いため、多くの技術者は VGG ベースのネットワークを好んで使っています。\n",
    "\n",
    "\n",
    "| VGG9          |\n",
    "| ------------- |\n",
    "| conv3-64      |\n",
    "| conv3-64      |\n",
    "| max3          |\n",
    "|               |\n",
    "| conv3-96      |\n",
    "| conv3-96      |\n",
    "| max3          |\n",
    "|               |\n",
    "| conv3-128     |\n",
    "| conv3-128     |\n",
    "| max3          |\n",
    "|               |\n",
    "| FC-1024       |\n",
    "| FC-1024       |\n",
    "|               |\n",
    "| FC-10         |\n",
    "\n",
    "\n",
    "上図は、重みのある層が全部で9層ある VGG ベースのモデル図になります。\n",
    "こちらを参照しながら VGG モデルを作ってみましょう。また、作ったモデルで学習・テストをし、認識精度の違いを比べてみましょう！\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_vgg9_model(input, out_dims):\n",
    "    \n",
    "    `````````````````\n",
    "    ここに書いてみよう\n",
    "    `````````````````\n",
    "    return model(input)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [cntk-py35]",
   "language": "python",
   "name": "Python [cntk-py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
